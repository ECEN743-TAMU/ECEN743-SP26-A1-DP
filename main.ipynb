{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNxeavk4-JW4"
      },
      "source": [
        "# ECEN743 Spring 2026 - Assignment 1\n",
        "## Tabular RL Algorithms\n",
        "\n",
        "In this assignment, you will solve the FrozenLake-v0 environment from [Gymnasium](https://gymnasium.farama.org/). You will be using this helper file to answer questions in your assignment.\n",
        "\n",
        "**Note that you do not need to start from the scratch. Only write your code between the following lines. Do not modify other parts.**  \n",
        "\"### YOUR CODE HERE\"  \n",
        "\"### END OF YOUR CODE\"\n",
        "\n",
        "## Introduction of the FrozenLake Environment\n",
        "\n",
        "Frozen lake involves crossing a frozen lake from start to goal without falling into any holes by walking over the frozen lake. The player may not always move in the intended direction due to the slippery nature of the frozen lake. The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment. Holes in the ice are distributed in set locations using a pre-determined map, and the player makes moves until they reach the goal or fall in a hole. The map is given below for your reference\n",
        "\n",
        "        SFFF\n",
        "        FHFH\n",
        "        FFFH\n",
        "        HFFG\n",
        "    S : starting point, safe\n",
        "    F : frozen surface, safe\n",
        "    H : hole, fall to your doom\n",
        "    G : goal, where the frisbee is located\n",
        "    \n",
        "    \n",
        "### Action Space\n",
        "The player/agent can take 4 discrete actions, in the range {0,3}\n",
        "* 0: Move left\n",
        "* 1: Move down\n",
        "* 2: Move right\n",
        "* 3: Move up\n",
        "\n",
        "\n",
        "### State Space\n",
        "The environment consists of 16 states. The state is a value representing the playerâ€™s current position as current_row * nrows + current_col (where both the row and col start at 0).\n",
        "For example, the goal position in the 4x4 map can be calculated as follows: 3 * 4 + 3 = 15.\n",
        "\n",
        "\n",
        "### Starting State\n",
        "The episode starts with the player in state [0] (location [0, 0]).\n",
        "\n",
        "\n",
        "### Rewards\n",
        "\n",
        "* Reach goal: +1\n",
        "* Reach hole: 0\n",
        "* Reach frozen: 0\n",
        "\n",
        "### Episode End\n",
        "The episode ends if the following happens:\n",
        "#### 1.Termination:\n",
        "* The player moves into a hole.\n",
        "* The player reaches the goal at max(nrow) * max(ncol) - 1 (location [max(nrow)-1, max(ncol)-1]).\n",
        "\n",
        "#### 2.Truncation:\n",
        "* The length of the episode is 100 for 4x4 environment.\n",
        "\n",
        "For more info refer to source: https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
        "\n",
        "### The Environment Parameters\n",
        "* Use discount factor, $\\gamma = 0.9$\n",
        "* The environment is slippery, ie., the transition kernel is stochastic.\n",
        "* The transition kernel P is a dictionary.\n",
        "* P[state][action] is tuples with (probability, nextstate, reward, terminal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NX3JA4C-JW6"
      },
      "source": [
        "**Run the following initializer. Make sure the path points to our rl_env.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayYA8wMF-JW6",
        "outputId": "cdb096d3-1d55-4572-828e-22733255622f"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(sys.executable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IZTMDKB-JW6"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "%matplotlib inline\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LogNorm\n",
        "import numpy as np\n",
        "from matplotlib.patches import Rectangle, Polygon\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Image\n",
        "from PIL import Image as PILImage\n",
        "import io\n",
        "\n",
        "env = gym.make('FrozenLake-v1', render_mode=\"rgb_array\", desc=None, map_name=\"4x4\", is_slippery=True)\n",
        "\n",
        "while hasattr(env, 'env'):\n",
        "    env = env.env\n",
        "\n",
        "class FrozenLakeDisplay:\n",
        "    def __init__(self, size=4):\n",
        "        self.grid_size = size\n",
        "        self.map = [\n",
        "            ['S', 'F', 'F', 'F'],\n",
        "            ['F', 'H', 'F', 'H'],\n",
        "            ['F', 'F', 'F', 'H'],\n",
        "            ['H', 'F', 'F', 'G']\n",
        "        ]\n",
        "\n",
        "    def create_triangles(self, x, y):\n",
        "        center = (x + 0.5, y + 0.5)\n",
        "        nw = (x, y + 1)\n",
        "        ne = (x + 1, y + 1)\n",
        "        se = (x + 1, y)\n",
        "        sw = (x, y)\n",
        "        \n",
        "        triangles = {\n",
        "            'up': [center, nw, ne],\n",
        "            'right': [center, ne, se],\n",
        "            'down': [center, sw, se],\n",
        "            'left': [center, nw, sw]\n",
        "        }\n",
        "        return triangles\n",
        "\n",
        "    def get_text_positions(self, x, y):\n",
        "        positions = {\n",
        "            'up': (x + 0.5, y + 0.8),\n",
        "            'right': (x + 0.8, y + 0.5),\n",
        "            'down': (x + 0.5, y + 0.2),\n",
        "            'left': (x + 0.2, y + 0.5)\n",
        "        }\n",
        "        return positions\n",
        "    \n",
        "\n",
        "    def get_state_color(self, state_type):\n",
        "        colors = {\n",
        "            'S': (0, 0.8, 0, 1),    # Start - Green\n",
        "            'F': (0.15, 0.15, 0.15, 1),  # Frozen - Dark gray\n",
        "            'H': (0.3, 0.3, 0.3, 1),    # Hole - Gray\n",
        "            'G': (0, 0.8, 0, 1)     # Goal - Green\n",
        "        }\n",
        "        return colors.get(state_type, (0.15, 0.15, 0.15, 1))\n",
        "\n",
        "    def display_q_values(self, q_values):\n",
        "        fig, ax = plt.subplots(figsize=(12, 12))\n",
        "        ax.set_aspect('equal')\n",
        "\n",
        "        q_values = convert_q_array_to_dict(q_values)\n",
        "\n",
        "        for i in range(self.grid_size):\n",
        "            for j in range(self.grid_size):\n",
        "                state_type = self.map[i][j]\n",
        "                cell = Rectangle((j, self.grid_size-1-i), 1, 1, facecolor=self.get_state_color(state_type), edgecolor='white', linewidth=1)\n",
        "                ax.add_patch(cell)\n",
        "\n",
        "                if state_type in ['H', 'G']:\n",
        "                    # Add state label\n",
        "                    ax.text(j+0.5, self.grid_size-1-i+0.5,  state_type, ha='center', va='center', color='white', fontsize=10)\n",
        "                    continue\n",
        "\n",
        "                triangles = self.create_triangles(j, self.grid_size-1-i)\n",
        "                text_pos  = self.get_text_positions(j, self.grid_size-1-i)\n",
        "                max_value = float('-inf')\n",
        "                for direction in ['left','down', 'right', 'up']:\n",
        "                    max_value = max(max_value, q_values[(i, j)][direction])\n",
        "                \n",
        "                for direction in ['left','down', 'right', 'up']:\n",
        "                    value = q_values[(i, j)][direction]\n",
        "                    if value == max_value:\n",
        "                        color = (0, 0.8, 0, 1)\n",
        "                    else:\n",
        "                        color = (0.15, 0.15, 0.15, 1)   \n",
        "                    triangle = Polygon(triangles[direction],  facecolor=color,  edgecolor='white',  linewidth=1)\n",
        "                    ax.add_patch(triangle)\n",
        "                    tx, ty = text_pos[direction]\n",
        "                    ax.text(tx, ty, f\"{value:.3f}\",  ha='center', va='center', color='white', fontsize=8)\n",
        "\n",
        "        ax.set_xlim(-0.1, self.grid_size + 0.1)\n",
        "        ax.set_ylim(-0.1, self.grid_size + 0.1)\n",
        "        ax.axis('off')\n",
        "        plt.title('CURRENT Q-VALUES', pad=20, color='white')\n",
        "        ax.set_facecolor('black')\n",
        "        fig.patch.set_facecolor('black')\n",
        "        plt.show()\n",
        "\n",
        "def convert_q_array_to_dict(q_array):\n",
        "    q_dict = {}\n",
        "    actions = ['left','down', 'right', 'up']\n",
        "    size = 4\n",
        "    \n",
        "    for i in range(size):\n",
        "        for j in range(size):\n",
        "            idx = i * size + j\n",
        "            q_values = q_array[idx]\n",
        "            q_dict[(i, j)] = dict(zip(actions, q_values))\n",
        "    return q_dict\n",
        "\n",
        "\n",
        "display_q = FrozenLakeDisplay()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxJaM4Yi-JW7"
      },
      "source": [
        "## 1. Q-value iteration\n",
        "\n",
        "### Explanation of the parameters\n",
        "* `Q_prev` is the Q-value function from the previous iteration.\n",
        "* `Q_curr` is the Q-value function of current iteration. In each iteration, you need to use model, reward, and the Q-value function from previous iteration (`Q_prev`) to compute `Q_curr`.\n",
        "* You can use\n",
        "    * `n = np.array([n for (p,n,r,t) in env.P[s][a]])` to access the information of the possible next states if you pick action `a` in state `s`. **Note that for conciseness, `env.P[s][a]` omits entries that are not reachable from `(s,a)`;**\n",
        "    * `p = np.array([p for (p,n,r,t) in env.P[s][a]])` to access the transition probabilities at any state-action pair `(s,a)`. For example, in state `4`, if you choose to move right (`a=2`), then `env.P[4][2]` outputs:\n",
        "        ```\n",
        "        [(0.3333333333333333, 8, 0.0, False),\n",
        "         (0.3333333333333333, 5, 0.0, True),\n",
        "         (0.3333333333333333, 0, 0.0, False)]\n",
        "        ```\n",
        "        That is, there is a one third chance that we end up in either state `8`, `5`, or `0`. **This is very import since the indexing of this array is totally irrelavant to the indexing of your Q-value function array.** In this particular example, you need to update `Q_curr[4]` using `p[0]` and `Q_prev[8]`, `p[1]` and `Q_prev[5]`, `p[2]` and `Q_prev[0]` (Why?);\n",
        "    * `r = np.array([r for (p,n,r,t) in env.P[s][a]])` to access the reward.\n",
        "\n",
        "### Your Task\n",
        "1. Complete the Bellman update in Task 1 below. In particular, calculate the current Q-value function `Q_curr` using the previous Q-value function `Q_prev`.\n",
        "2. Compute the optimal value function `Vopt` and the optimal policy `Policyopt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "zAuQfBow-JW7",
        "outputId": "eed72fb6-9965-4246-e151-208b1c1d4987"
      },
      "outputs": [],
      "source": [
        "no_of_actions = env.action_space.n\n",
        "no_of_states = env.observation_space.n\n",
        "epsilon = 1e-5\n",
        "gamma  = 0.9\n",
        "Q_prev = np.array([[0.0]*no_of_actions for i in range(no_of_states)])\n",
        "Q_delta = []\n",
        "delta = 1\n",
        "\n",
        "\n",
        "while(delta > epsilon):\n",
        "    Q_curr = np.array([[0.0]*no_of_actions for i in range(no_of_states)])\n",
        "    ### YOUR CODE HERE\n",
        "    # Update Q - Bellman Equation\n",
        "    # Task 1\n",
        "    ### END OF YOUR CODE\n",
        "    Q_prev = Q_curr.copy()\n",
        "\n",
        "# Task 2\n",
        "### YOUR CODE HERE\n",
        "\n",
        "V_opt      = []\n",
        "Policy_opt  = []\n",
        "\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5bSCEWx-JW7"
      },
      "source": [
        "### 1a. Optimal Policy and Value function\n",
        "\n",
        "You do not need to modify the code below but you have to run it before submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtwjuk7k-JW7",
        "outputId": "67d3e372-099b-4e15-9595-fb715ef053c6"
      },
      "outputs": [],
      "source": [
        "print(\"Optimal Value function: \")\n",
        "print(V_opt)\n",
        "print(\"Optimal Policy\")\n",
        "print(Policy_opt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLiJ3uxV-JW7"
      },
      "source": [
        "### 1b. Plot $||Q_k - Q_{k-1}||$\n",
        "\n",
        "You do not need to modify the code below but you have to run it before submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "906ecpVd-JW7",
        "outputId": "90509bb5-b800-4e1f-97c4-91044f65e853"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(Q_delta)),Q_delta)\n",
        "plt.title(\"Q-value Iteration\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"$||Q_k - Q_{k-1}||$\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3BkYcMn-JW8"
      },
      "source": [
        "### 1c. Heat map\n",
        "\n",
        "You do not need to modify the code below but you have to run it before submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "id": "I0c9SgWZ-JW8",
        "outputId": "1bc0a5f1-d8f7-4531-f699-f61380bc9d50"
      },
      "outputs": [],
      "source": [
        "display_q.display_q_values(Q_curr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKTy-ny6-JW8"
      },
      "source": [
        "## 2. Policy Evaluation  \n",
        "\n",
        "### 2a. linear system of equations\n",
        "\n",
        "### Hint: one way to do this\n",
        "1. Compute `P_opt` which is a $|\\mathcal{S}|\\times|\\mathcal{S}|$ matrix where the entry in the $i$-th row, $j$-th column represents the probability of going from state `i` to state `j` by executing the optimal policy obtained by QVI in Problem 1.\n",
        "\n",
        "2. Compute `r_opt` which is a $|\\mathcal{S}|$-dimensional vector whose $i$-th element is $$\\mathbb{E}_{a\\sim \\pi^*(\\cdot\\mid s)}\\mathbb{E}_{s'\\sim P(\\cdot\\mid s,a)}[r(s,a,s')] \\stackrel{(a)}{=} \\mathbb{E}_{s'\\sim P(\\cdot\\mid s,\\pi^*(s))}[r(s,\\pi^*(s),s')]$$, where $(a)$ is because $\\pi^*$ is deterministic.\n",
        "\n",
        "3. Recall the Bellman consistency equation, for any policy $\\pi$, we have $V^{\\pi} = (I-\\gamma P^{\\pi})^{-1} r^{\\pi}$. Rearrange the terms, we can look at the system of linear equations, $(I-\\gamma P^{\\pi}) V^{\\pi} = r^{\\pi}$, and solve for $V^\\pi$.\n",
        "\n",
        "4. For $\\pi_{\\mathrm{unif}}$, repeat the steps above but be careful since the policy is no longer deterministic. You need to do extra work in step 2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tOiiDUT-JW8"
      },
      "outputs": [],
      "source": [
        "# Optimal policy\n",
        "I = np.identity(no_of_states)\n",
        "P_opt = np.zeros((no_of_states,no_of_states))\n",
        "r_opt = np.zeros(no_of_states)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "     \n",
        "### END OF YOUR CODE\n",
        "\n",
        "# Uniform policy\n",
        "I = np.identity(no_of_states)\n",
        "P_unif = np.zeros((no_of_states,no_of_states))\n",
        "r_unif = np.zeros(no_of_states)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END OF YOUR CODE\n",
        "\n",
        "print(\"Value function under optimal policy: \")\n",
        "### YOUR CODE HERE\n",
        "# print your value function for the optimal policy\n",
        "### END OF YOUR CODE\n",
        "\n",
        "print(\"Value function under uniformly random policy:\")\n",
        "### YOUR CODE HERE\n",
        "# print your value function for the uniform policy\n",
        "### END OF YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZiXZyTa-JW8"
      },
      "source": [
        "### 2b. Iterative Method\n",
        "\n",
        "Recall the Bellman consistency equation that for any policy $\\pi$, we have\n",
        "$$\n",
        "V^\\pi(s) = \\mathbb{E}_{a\\sim \\pi(\\cdot\\mid s)}\\mathbb{E}_{s'\\sim P(\\cdot\\mid s,a)}[r(s,a,s') + \\gamma V^\\pi(s')].\n",
        "$$\n",
        "\n",
        "Please keep `epsilon` unchanged. Remember to update the `delta` in the while loop to reflect the current convergency of the contraction mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "bGHbl8jb-JW8",
        "outputId": "481006af-2fb1-4299-ebd8-4b6596a239e8"
      },
      "outputs": [],
      "source": [
        "epsilon = 1e-10\n",
        "#optimal policy\n",
        "V_prev = np.array([0.0]*no_of_states)\n",
        "delta = 1\n",
        "while(delta > epsilon):\n",
        "    ### YOUR CODE HERE\n",
        "    # V_curr = ?\n",
        "    # delta = compare V_prev and V_curr\n",
        "    ### END OF YOUR CODE\n",
        "    V_prev = V_curr\n",
        "\n",
        "print(\"Value function under optimal policy: \")\n",
        "print(V_prev)\n",
        "\n",
        "#uniform policy\n",
        "V_prev = np.array([0.0]*no_of_states)\n",
        "delta = 1\n",
        "while(delta > epsilon):\n",
        "    ### YOUR CODE HERE\n",
        "    # V_curr = ?\n",
        "    # delta = compare V_prev and V_curr\n",
        "    ### END OF YOUR CODE\n",
        "    V_prev = V_curr\n",
        "    \n",
        "print(\"Value function under uniform policy: \")\n",
        "print(V_prev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We6CAvVK-JW8"
      },
      "source": [
        "### 2c. Comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9VQFn1B-JW8"
      },
      "source": [
        "**Write your answer below.**  \n",
        "Answer:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLplKgki-JW9"
      },
      "source": [
        "## 3. Policy Iteration\n",
        "\n",
        "### Hint: one way to do this\n",
        "1. In the policy evaluation step, calculate `P_pol` matrix. This should be identical to Problem **2a**.\n",
        "2. Calculate `r_pol` which is a $|\\mathcal{S}|$-dimensional vector. You should have\n",
        "$r_\\mathrm{pol}[s] = \\mathbb{E}_{a\\sim\\pi_{\\mathrm{prev}}(\\cdot\\mid s)} \\mathbb{E}_{s'\\sim P(\\cdot\\mid s,a)} [r(s,a,s')]. $\n",
        "Note that this should be just one line. Refer to **Explanation of the parameters** in **Problem 1** for `p`, `n`, and `r`.  \n",
        "3. Compute `V_curr` using `P_pol` and `r_pol`. It represents the value function of the current policy $\\pi_k$.\n",
        "4. Compute new policy $\\pi_{k+1}$ using the value function of $\\pi_k$. In particular, you need to use `V_pol_prev` to compute `pi_curr`.\n",
        "\n",
        "**Note, for this problem, you can remove everything below and start from the scratch. **\n",
        "\n",
        "**However, you have to save your optimal policy as `piopt_politer` and your optimal value function as `Vopt_politer`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pi_prev = np.random.randint(0,no_of_actions,size=no_of_states)\n",
        "V_pi_delta = []\n",
        "V_pol_prev = np.array([0.0]*no_of_states)\n",
        "\n",
        "epsilon_o = 1e-5\n",
        "epsilon_i = 1e-10\n",
        "\n",
        "delta_o = 1\n",
        "while(delta_o > 0):\n",
        "    ### YOUR CODE HERE\n",
        "    # some prep work for policy evaluation\n",
        "    ###\n",
        "    delta_i = 1\n",
        "    while(delta_i > epsilon_i):\n",
        "        ### YOUR CODE HERE\n",
        "        # V_curr = r_pol + gamma*(P_pol @ V_pol_prev_i)\n",
        "        ### END OF YOUR CODE\n",
        "        # delta_i = compare V_curr and V_pol_prev_i\n",
        "        V_pol_prev_i = V_curr  # update while loop\n",
        "        \n",
        "    V_pol_prev = V_pol_prev_i\n",
        "    \n",
        "    pi_curr = np.zeros((no_of_states,), dtype=int)\n",
        "    ### YOUR CODE HERE\n",
        "    # design your own policy improvement steps\n",
        "    ### END OF YOUR CODE\n",
        "    # delta_o = compare pi_curr and pi_prev\n",
        "    #print(delta_o)\n",
        "    pi_prev = pi_curr  # update while loop\n",
        "\n",
        "plt.plot(range(len(V_pi_delta)),V_pi_delta)\n",
        "plt.title(\"Policy Iteration\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"$||V_{\\pi_k} - V_{\\pi_{k-1}}||$\")\n",
        "plt.show()\n",
        "\n",
        "# Remember to save your optimal policy as `piopt_politer` and your optimal value function as `Vopt_politer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pg3nx66-JW9"
      },
      "source": [
        "### 3a. Optimal Policy and Value function\n",
        "\n",
        "You do not need to modify the code below but you have to run it before submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uLVtxWlo-JW9"
      },
      "outputs": [],
      "source": [
        "print(\"Optimal Value function: \")\n",
        "print(Vopt_politer)\n",
        "print(\"Optimal Policy\")\n",
        "print(piopt_politer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run below cell for policy animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIjTG7qTRZpM"
      },
      "outputs": [],
      "source": [
        "done = False\n",
        "frames = []\n",
        "episodes = 0\n",
        "state, _ = env.reset()\n",
        "while not done and episodes < 100:\n",
        "    action = pi_prev[state]\n",
        "    state, reward, done, truncated, info = env.step(action)\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    episodes += 1\n",
        "\n",
        "gif_buffer = io.BytesIO()\n",
        "pil_images = [PILImage.fromarray(frame) for frame in frames] \n",
        "pil_images[0].save(gif_buffer, format='GIF', save_all=True, append_images=pil_images[1:], loop=0, duration=500)  \n",
        "gif_buffer.seek(0)\n",
        "display(Image(data=gif_buffer.read(), format='png'))\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ_KaFeR-JW9"
      },
      "source": [
        "### 3b. Compare the convergence of QVI and PI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDPM_nLf-JW9"
      },
      "source": [
        "**Write your answer below.**  \n",
        "Answer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4a. Run Q-value iteration or Policy Iteration on Cliff walking and visualize the policy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = gym.make('CliffWalking-v1', render_mode = 'rgb_array',  is_slippery = 'True')\n",
        "while hasattr(env, 'env'):\n",
        "    env = env.env\n",
        "no_of_actions = env.action_space.n\n",
        "no_of_states = env.observation_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### YOUR CODE ### \n",
        "# Your Choice for Policy learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Run below cell for policy animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "done = False\n",
        "frames = []\n",
        "episodes = 0\n",
        "state, _ = env.reset()\n",
        "while not done and episodes < 500:\n",
        "    action = pi_prev[state]\n",
        "    state, reward, done, truncated, info = env.step(action)\n",
        "    frame = env.render()\n",
        "    frames.append(frame)\n",
        "    episodes += 1\n",
        "\n",
        "gif_buffer = io.BytesIO()\n",
        "pil_images = [PILImage.fromarray(frame) for frame in frames] \n",
        "pil_images[0].save(gif_buffer, format='GIF', save_all=True, append_images=pil_images[1:], loop=0, duration=500)  \n",
        "gif_buffer.seek(0)\n",
        "display(Image(data=gif_buffer.read(), format='png'))\n",
        "env.close()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
